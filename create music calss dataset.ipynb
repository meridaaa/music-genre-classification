{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "ozRJv81zeq2E",
        "outputId": "419c5662-4690-4ea3-c18a-88aa5cb9715e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-c84743e1f4cb>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    if dirpath is not dataset_apth:\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import librosa\n",
        "import json\n",
        "import math\n",
        "\n",
        "# DATASET_PATH\n",
        "# JSON_PATH\n",
        "SAMPLE_RATE = 22050\n",
        "DURATION = 30\n",
        "SAMPLE_PER_TRACK = SAMPLE_RATE * DURATION\n",
        "\n",
        "def save_mfcc(dataset, json_path, n_mfcc=13, n_fft=2048, hop_length=512, num_segments=5):\n",
        "  data={\n",
        "      \"mapping\":[],\n",
        "      \"mfcc\": [],\n",
        "      \"labels\":[]\n",
        "  }\n",
        "\n",
        "  num_samples_per_segment = int(SAMPLE_PER_TRACK / num_segments)\n",
        "  expected_num_mfcc_vectors_per_seg = math.ceil(num_samoles_per_segment / hop_length)\n",
        "\n",
        "  for i, (dirpath, dirnames, filenames) in enumarate(os.walk(dataset_path)):\n",
        "    if dirpath is not dataset_apth:\n",
        "      dirpath_components = dirpath.split('/')\n",
        "      semantic_label = dirpath_components[-1]\n",
        "      data['mapping'].append(semantic_label)\n",
        "      print('\\n processing {}'.format{semantic_label})\n",
        "\n",
        "      for f in filenames:\n",
        "        #loading ausio file\n",
        "        file_path = os.apth.join(dirpaht, f)\n",
        "        signal, sr = librosa.load(file_path, sr=22050)\n",
        "        \n",
        "        for s in range(num_segmets):\n",
        "          start_sample = num_samples_per_segment * s\n",
        "          finish_sample = start_sample + num_samples_per_segment\n",
        "\n",
        "          \n",
        "          mfcc = librosa.feature.mfcc(signal[start_sample:finish_sample],\n",
        "                                      sr=sr,\n",
        "                                      n_fft=n_fft,\n",
        "                                      m_mfcc=m_fcc,\n",
        "                                      hop_length=hop_length)\n",
        "          mfcc = mfcc.T\n",
        "          if len(mfcc) == expected_num_mfcc_vectors_per_seg:\n",
        "            data['mfcc'].append(mfcc.tolist())\n",
        "            data['labels'].append(i-1)\n",
        "            print('{}, segment'.format(file_path, s+1))\n",
        "  with open(json_path, 'w') as fp:\n",
        "    json.dump(data, fp, indent=4)\n",
        "\n",
        "save_mfcc(DATASET_PATH), JSON_PATH, num_segments=10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "P"
      ],
      "metadata": {
        "id": "HHAq_xrrzG1q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}